{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers # type: ignore\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input # type: ignore\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau # type: ignore\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Model # type: ignore\n",
    "from tensorflow.keras.utils import to_categorical # type: ignore\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import os\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FUNCTION DECLARATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_combine(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_area_df = pd.read_csv(\"dataset/Machine-Area.csv\")\n",
    "machine_list_df = pd.read_csv(\"dataset/Machine-List.csv\")\n",
    "area_list_df = pd.read_csv(\"dataset/Area-List.csv\")\n",
    "machine_area_df['Last Maintenance'] = pd.to_datetime(machine_area_df['Last Maintenance'])\n",
    "machine_area_filtered_df = machine_area_df.sort_values(\n",
    "    by=['ID_Area', 'ID_Mesin', 'Last Maintenance'], \n",
    "    ascending=[True, True, False]\n",
    ").drop_duplicates(subset=['ID_Area', 'ID_Mesin'], keep='first')\n",
    "\n",
    "def apply_combine(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret: pd.DataFrame = df.copy()\n",
    "    \n",
    "    ret = pd.merge(ret, area_list_df, on='ID_Area', how='left')\n",
    "    ret = pd.merge(ret, machine_list_df, left_on='Machine', right_on='ID_Mesin', how='left')\n",
    "    ret = pd.merge(ret, machine_area_filtered_df, left_on=['Machine', 'ID_Area'], right_on=['ID_Mesin', 'ID_Area'], how='left')\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_fill_na(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fill_na(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    missing = ret.isnull().sum()\n",
    "\n",
    "    # missing_percentage = missing / len(ret) * 100\n",
    "    # columns_to_impute = missing_percentage[missing_percentage < 5].index\n",
    "    columns_to_impute = ret.columns\n",
    "\n",
    "    for col in columns_to_impute:\n",
    "        if ret[col].dtype in ['float64', 'int64']:\n",
    "            ret[col] = ret[col].fillna(ret[col].median())\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_scaler(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "def apply_scaler(df:pd.DataFrame, train: bool) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    numerical_cols = [col for col in df.columns if ret[col].dtype in ['float64', 'int64']]\n",
    "    if train:\n",
    "        scaler.fit(ret[numerical_cols])\n",
    "    ret[numerical_cols] = scaler.transform(ret[numerical_cols])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aply_reduce_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_reduce_dim(df: pd.DataFrame, dim: int, epochs: int = 25) -> tuple:\n",
    "    ret = df.copy()\n",
    "    numerical_cols = [col for col in ret.columns if ret[col].dtype in ['float64', 'int64']]\n",
    "    cols_to_reduce = [col for col in numerical_cols if col != 'Age']\n",
    "    X = ret[cols_to_reduce]\n",
    "    input_dim = X.shape[1]\n",
    "\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(dim, activation='relu')(input_layer)\n",
    "    decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    autoencoder.fit(X, X, epochs=epochs, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "\n",
    "    encoder_model = Model(inputs=input_layer, outputs=encoder)\n",
    "    X_reduced = encoder_model.predict(X)\n",
    "    X_reduced = pd.DataFrame(X_reduced, columns=[f'feature_{i}' for i in range(dim)])\n",
    "    ret.drop(cols_to_reduce, axis=1, inplace=True)\n",
    "    ret = pd.concat([ret, X_reduced], axis=1)\n",
    "    return ret, encoder_model, cols_to_reduce\n",
    "\n",
    "\n",
    "def apply_reduce_dim_gpu(df: pd.DataFrame, dim: int, epochs: int = 25) -> tuple:\n",
    "    ret = df.copy()\n",
    "    numerical_cols = [col for col in ret.columns if ret[col].dtype in ['float64', 'int64']]\n",
    "    cols_to_reduce = [col for col in numerical_cols if col != 'Age']\n",
    "    X = ret[cols_to_reduce]\n",
    "    input_dim = X.shape[1]\n",
    "\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(dim, activation='relu')(input_layer)\n",
    "    decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    autoencoder.fit(X, X, epochs=epochs, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "\n",
    "    encoder_model = Model(inputs=input_layer, outputs=encoder)\n",
    "    X_reduced = encoder_model.predict(X)\n",
    "    X_reduced = pd.DataFrame(X_reduced, columns=[f'feature_{i}' for i in range(dim)])\n",
    "    ret.drop(cols_to_reduce, axis=1, inplace=True)\n",
    "    ret = pd.concat([ret, X_reduced], axis=1)\n",
    "    return ret, encoder_model, cols_to_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_time_encoding(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_time_encoding(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    ret['Last Maintenance'] = pd.to_datetime(ret['Last Maintenance'])\n",
    "    ret['timestamp'] = pd.to_datetime(ret['timestamp'])\n",
    "    ret['days_since_last_maintenance'] = (ret['timestamp'] - ret['Last Maintenance']).dt.days\n",
    "    ret.drop(['Last Maintenance', 'timestamp'], axis=1, inplace=True)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_one_hot_encode(df, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_one_hot_encode(df: pd.DataFrame, cols):\n",
    "    if df[cols].isnull().any():\n",
    "        df[cols] = df[cols].fillna(\"Missing\")\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    onehot_encoded = encoder.fit_transform(df[[cols]])\n",
    "    encoded_df = pd.DataFrame(onehot_encoded, columns=encoder.get_feature_names_out([cols]), index = df.index)\n",
    "    for column in encoded_df.columns:\n",
    "        encoded_df[column] = pd.Categorical(encoded_df[column])\n",
    "    df_encoded = pd.concat([df.drop(columns=cols), encoded_df], axis=1)\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_feature_selection(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_feature_selection(df: pd.DataFrame):\n",
    "    ret = df.copy()\n",
    "    ret = ret.drop(columns=[\n",
    "        'ID_Area', 'ID_Mesin_x', 'Machine', 'Breakdown Category', 'Area', 'ID_Transaction',\n",
    "        'ID_Mesin_y', 'Mesin_x', 'Mesin_y', 'Country Machine_x', 'Country Machine_y'])\n",
    "    if 'Breakdown Category' in ret.columns:\n",
    "        ret = ret.drop(columns=['Breakdown Category'])\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_fill_category(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fill_category(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    for col in ret.select_dtypes(include=['category']).columns:\n",
    "            ret[col] = ret[col].cat.add_categories([-1])\n",
    "            ret[col] = ret[col].fillna(-1)\n",
    "    ret = ret.fillna(-1)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_smote(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smote(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    target = ret['Status'].map({'Normal': 0, 'Warning': 1, 'Breakdown': 2})\n",
    "    ret = ret.drop('Status', axis=1)\n",
    "    smote = SMOTE(random_state=42, n_jobs=-1)\n",
    "    X_resampled, y_resampled = smote.fit_resample(ret, target)\n",
    "    X_resampled = pd.DataFrame(X_resampled, columns=ret.columns)\n",
    "    X_resampled['Status'] = y_resampled\n",
    "    return X_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 1.0517 - val_loss: 0.9404\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:370: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "for chunk in pd.read_csv(\"dataset/train.csv\", chunksize=100_000):\n",
    "    df = chunk.copy()\n",
    "    df = apply_combine(df)\n",
    "    df = apply_fill_na(df)\n",
    "    df = apply_scaler(df, train=True)\n",
    "    try:\n",
    "        df, encoder_model, cols_to_reduce = apply_reduce_dim_gpu(df, 3)\n",
    "    except:\n",
    "        df, encoder_model, cols_to_reduce = apply_reduce_dim(df, 3)\n",
    "    df = apply_time_encoding(df)\n",
    "    df = apply_one_hot_encode(df, 'Priority')\n",
    "    df = apply_one_hot_encode(df, 'Status Sparepart')\n",
    "    df = apply_one_hot_encode(df, 'Power_Backup')\n",
    "    df = apply_feature_selection(df)\n",
    "    df = apply_fill_category(df)\n",
    "    df = apply_smote(df)\n",
    "\n",
    "    X = df.drop(columns='Status')\n",
    "    y = df['Status']\n",
    "\n",
    "    rf = RandomForestClassifier(n_jobs=-1)\n",
    "    rf.fit(X, y)\n",
    "    models.append(rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 saved as saved_models/model_0.pkl\n"
     ]
    }
   ],
   "source": [
    "def save_model():\n",
    "    save_dir = 'saved_models/'\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for idx, model in enumerate(models):\n",
    "        model_filename = os.path.join(save_dir, f'model_{idx}.pkl')\n",
    "        \n",
    "        with open(model_filename, 'wb') as model_file:\n",
    "            pickle.dump(model, model_file)\n",
    "\n",
    "        print(f'Model {idx} saved as {model_filename}')\n",
    "\n",
    "# save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models_from_folder() -> list:\n",
    "    folder_path = \"saved_models/\"\n",
    "    models = []\n",
    "\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder '{folder_path}' does not exist.\")\n",
    "        return models\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pkl'): \n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            with open(file_path, 'rb') as model_file:\n",
    "                model = pickle.load(model_file)\n",
    "                models.append(model)\n",
    "                print(f\"Loaded model from {file_path}\")\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PREDICTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
