{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FUNCTION DECLARTAION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_combine(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_area_df = pd.read_csv(\"dataset/Machine-Area.csv\")\n",
    "machine_list_df = pd.read_csv(\"dataset/Machine-List.csv\")\n",
    "area_list_df = pd.read_csv(\"dataset/Area-List.csv\")\n",
    "machine_area_df['Last Maintenance'] = pd.to_datetime(machine_area_df['Last Maintenance'])\n",
    "machine_area_filtered_df = machine_area_df.sort_values(\n",
    "    by=['ID_Area', 'ID_Mesin', 'Last Maintenance'], \n",
    "    ascending=[True, True, False]\n",
    ").drop_duplicates(subset=['ID_Area', 'ID_Mesin'], keep='first')\n",
    "\n",
    "def apply_combine(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret: pd.DataFrame = df.copy()\n",
    "    \n",
    "    ret = pd.merge(ret, area_list_df, on='ID_Area', how='left')\n",
    "    ret = pd.merge(ret, machine_list_df, left_on='Machine', right_on='ID_Mesin', how='left')\n",
    "    ret = pd.merge(ret, machine_area_filtered_df, left_on=['Machine', 'ID_Area'], right_on=['ID_Mesin', 'ID_Area'], how='left')\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_fill_na(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fill_na(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    missing = ret.isnull().sum()\n",
    "\n",
    "    # missing_percentage = missing / len(ret) * 100\n",
    "    # columns_to_impute = missing_percentage[missing_percentage < 5].index\n",
    "    columns_to_impute = ret.columns\n",
    "\n",
    "    for col in columns_to_impute:\n",
    "        if ret[col].dtype in ['float64', 'int64']:\n",
    "            ret[col] = ret[col].fillna(ret[col].median())\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_scaler(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "def apply_scaler(df:pd.DataFrame, train: bool) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    numerical_cols = [col for col in df.columns if ret[col].dtype in ['float64', 'int64']]\n",
    "    if train:\n",
    "        scaler.fit(ret[numerical_cols])\n",
    "    ret[numerical_cols] = scaler.transform(ret[numerical_cols])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aply_reduce_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_reduce_dim(df: pd.DataFrame, dim: int, epochs: int = 25) -> tuple:\n",
    "    ret = df.copy()\n",
    "    numerical_cols = [col for col in ret.columns if ret[col].dtype in ['float64', 'int64']]\n",
    "    cols_to_reduce = [col for col in numerical_cols if col != 'Age']\n",
    "    X = ret[cols_to_reduce]\n",
    "    input_dim = X.shape[1]\n",
    "\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(dim, activation='relu')(input_layer)\n",
    "    decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    autoencoder.fit(X, X, epochs=epochs, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "\n",
    "    encoder_model = Model(inputs=input_layer, outputs=encoder)\n",
    "    X_reduced = encoder_model.predict(X)\n",
    "    X_reduced = pd.DataFrame(X_reduced, columns=[f'feature_{i}' for i in range(dim)])\n",
    "    ret.drop(cols_to_reduce, axis=1, inplace=True)\n",
    "    ret = pd.concat([ret, X_reduced], axis=1)\n",
    "    return ret, encoder_model, cols_to_reduce\n",
    "\n",
    "\n",
    "def apply_reduce_dim_gpu(df: pd.DataFrame, dim: int, epochs: int = 25) -> tuple:\n",
    "    ret = df.copy()\n",
    "    numerical_cols = [col for col in ret.columns if ret[col].dtype in ['float64', 'int64']]\n",
    "    cols_to_reduce = [col for col in numerical_cols if col != 'Age']\n",
    "    X = ret[cols_to_reduce]\n",
    "    input_dim = X.shape[1]\n",
    "\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(dim, activation='relu')(input_layer)\n",
    "    decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    autoencoder.fit(X, X, epochs=epochs, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "\n",
    "    encoder_model = Model(inputs=input_layer, outputs=encoder)\n",
    "    X_reduced = encoder_model.predict(X)\n",
    "    X_reduced = pd.DataFrame(X_reduced, columns=[f'feature_{i}' for i in range(dim)])\n",
    "    ret.drop(cols_to_reduce, axis=1, inplace=True)\n",
    "    ret = pd.concat([ret, X_reduced], axis=1)\n",
    "    return ret, encoder_model, cols_to_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_time_encoding(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_time_encoding(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    ret['Last Maintenance'] = pd.to_datetime(ret['Last Maintenance'])\n",
    "    ret['timestamp'] = pd.to_datetime(ret['timestamp'])\n",
    "    ret['days_since_last_maintenance'] = (ret['timestamp'] - ret['Last Maintenance']).dt.days\n",
    "    ret.drop(['Last Maintenance', 'timestamp'], axis=1, inplace=True)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_one_hot_encode(df, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_one_hot_encode(df: pd.DataFrame, cols):\n",
    "    if df[cols].isnull().any():\n",
    "        df[cols] = df[cols].fillna(\"Missing\")\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    onehot_encoded = encoder.fit_transform(df[[cols]])\n",
    "    encoded_df = pd.DataFrame(onehot_encoded, columns=encoder.get_feature_names_out([cols]), index = df.index)\n",
    "    for column in encoded_df.columns:\n",
    "        encoded_df[column] = pd.Categorical(encoded_df[column])\n",
    "    df_encoded = pd.concat([df.drop(columns=cols), encoded_df], axis=1)\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_feature_selection(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_feature_selection(df: pd.DataFrame):\n",
    "    ret = df.copy()\n",
    "    ret = ret.drop(columns=[\n",
    "        'ID_Area', 'ID_Mesin_x', 'Machine', 'Breakdown Category', 'Area', 'ID_Transaction',\n",
    "        'ID_Mesin_y', 'Mesin_x', 'Mesin_y', 'Country Machine_x', 'Country Machine_y'])\n",
    "    if 'Breakdown Category' in ret.columns:\n",
    "        ret = ret.drop(columns=['Breakdown Category'])\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_fill_category(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fill_category(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    for col in ret.select_dtypes(include=['category']).columns:\n",
    "            ret[col] = ret[col].cat.add_categories([-1])\n",
    "            ret[col] = ret[col].fillna(-1)\n",
    "    ret = ret.fillna(-1)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_smote(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smote(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    target = ret['Status'].map({'Normal': 0, 'Warning': 1, 'Breakdown': 2})\n",
    "    ret = ret.drop('Status', axis=1)\n",
    "    smote = SMOTE(random_state=42, n_jobs=-1)\n",
    "    X_resampled, y_resampled = smote.fit_resample(ret, target)\n",
    "    X_resampled = pd.DataFrame(X_resampled, columns=ret.columns)\n",
    "    X_resampled['Status'] = y_resampled\n",
    "    return X_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - loss: 1.0545 - val_loss: 0.9382\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'push'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [61], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier()\n\u001b[0;32m     24\u001b[0m rf\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[1;32m---> 25\u001b[0m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush\u001b[49m(rf)    \n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'push'"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "for chunk in pd.read_csv(\"dataset/train.csv\", chunksize=100_000):\n",
    "    df = chunk.copy()\n",
    "    df = apply_combine(df)\n",
    "    df = apply_fill_na(df)\n",
    "    df = apply_scaler(df, train=True)\n",
    "    try:\n",
    "        df, encoder_model, cols_to_reduce = apply_reduce_dim_gpu(df, 3, 1)\n",
    "    except:\n",
    "        df, encoder_model, cols_to_reduce = apply_reduce_dim(df, 3, 1)\n",
    "    df = apply_time_encoding(df)\n",
    "    df = apply_one_hot_encode(df, 'Priority')\n",
    "    df = apply_one_hot_encode(df, 'Status Sparepart')\n",
    "    df = apply_one_hot_encode(df, 'Power_Backup')\n",
    "    df = apply_feature_selection(df)\n",
    "    df = apply_fill_category(df)\n",
    "    df = apply_smote(df)\n",
    "\n",
    "    X = df.drop(columns='Status')\n",
    "    y = df['Status']\n",
    "\n",
    "    rf = RandomForestClassifier(n_jobs=-1)\n",
    "    rf.fit(X, y)\n",
    "    models.append(rf)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          Warning\n",
       "1           Normal\n",
       "2           Normal\n",
       "3        Breakdown\n",
       "4        Breakdown\n",
       "           ...    \n",
       "99995    Breakdown\n",
       "99996      Warning\n",
       "99997    Breakdown\n",
       "99998       Normal\n",
       "99999       Normal\n",
       "Name: Status, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
