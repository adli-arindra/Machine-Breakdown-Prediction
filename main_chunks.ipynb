{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers # type: ignore\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization # type: ignore\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau # type: ignore\n",
    "from tensorflow.keras.models import load_model # type: ignore\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Model # type: ignore\n",
    "from tensorflow.keras.utils import to_categorical # type: ignore\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import os\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FUNCTION DECLARATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_combine(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_area_df = pd.read_csv(\"dataset/Machine-Area.csv\")\n",
    "machine_list_df = pd.read_csv(\"dataset/Machine-List.csv\")\n",
    "area_list_df = pd.read_csv(\"dataset/Area-List.csv\")\n",
    "machine_area_df['Last Maintenance'] = pd.to_datetime(machine_area_df['Last Maintenance'])\n",
    "maintenance_frequency = machine_area_df.groupby('ID_Mesin').size().rename('maintenance_count')\n",
    "machine_area_df = pd.merge(machine_area_df, maintenance_frequency, left_on='ID_Mesin', right_index=True, how='left')\n",
    "machine_area_filtered_df = machine_area_df.sort_values(\n",
    "    by=['ID_Area', 'ID_Mesin', 'Last Maintenance'], \n",
    "    ascending=[True, True, False]\n",
    ").drop_duplicates(subset=['ID_Area', 'ID_Mesin'], keep='first')\n",
    "\n",
    "def apply_combine(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret: pd.DataFrame = df.copy()\n",
    "    \n",
    "    ret = pd.merge(ret, area_list_df, on='ID_Area', how='left')\n",
    "    ret = pd.merge(ret, machine_list_df, left_on='Machine', right_on='ID_Mesin', how='left')\n",
    "    ret = pd.merge(ret, machine_area_filtered_df, left_on=['Machine', 'ID_Area'], right_on=['ID_Mesin', 'ID_Area'], how='left')\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_fill_na(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "_columns_to_keep = ['Last Maintenance', 'Status Sparepart', 'Power_Backup', 'Priority', 'Status']\n",
    "\n",
    "_columns_to_impute: list[str] = []\n",
    "_columns_to_drop: list[str] = []\n",
    "\n",
    "def apply_fill_na(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "\n",
    "    avg_timespan = 10\n",
    "    valid_rows = ret['ID_Mesin_x'].notnull() & ret['ID_Area'].notnull()\n",
    "\n",
    "    if valid_rows.any():\n",
    "        ret.loc[valid_rows, 'predicted_age'] = (\n",
    "            ret.loc[valid_rows]\n",
    "            .groupby(['ID_Mesin_x', 'ID_Area'])['Last Maintenance']\n",
    "            .transform('min')\n",
    "            .dt.year + avg_timespan\n",
    "        )\n",
    "        ret.loc[valid_rows, 'Age'] = ret.loc[valid_rows, 'Age'].fillna(ret.loc[valid_rows, 'predicted_age'])\n",
    "\n",
    "    ret.loc[~valid_rows, 'Age'] = np.nan\n",
    "\n",
    "    ret = ret.drop(columns=['predicted_age'], errors='ignore')\n",
    "\n",
    "    missing = ret.isnull().sum()\n",
    "    missing_percentage = missing / len(ret) * 100\n",
    "\n",
    "    columns_to_impute = missing_percentage[missing_percentage < 5].index\n",
    "    for col in columns_to_impute:\n",
    "        if ret[col].dtype in ['float64', 'int64']:\n",
    "            ret[col] = ret[col].fillna(ret[col].median())\n",
    "\n",
    "    missing_percentage = ret.isnull().sum() / len(ret) * 100\n",
    "    columns_to_drop = missing_percentage[missing_percentage > 5].index\n",
    "    filtered_columns_to_drop = [col for col in columns_to_drop if col not in _columns_to_keep]\n",
    "    ret = ret.drop(columns=filtered_columns_to_drop)\n",
    "\n",
    "    _columns_to_impute = columns_to_impute\n",
    "    _columns_to_drop = filtered_columns_to_drop\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fill_na_predict(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "\n",
    "    avg_timespan = 10\n",
    "    valid_rows = ret['ID_Mesin_x'].notnull() & ret['ID_Area'].notnull()\n",
    "\n",
    "    if valid_rows.any():\n",
    "        ret.loc[valid_rows, 'predicted_age'] = (\n",
    "            ret.loc[valid_rows]\n",
    "            .groupby(['ID_Mesin_x', 'ID_Area'])['Last Maintenance']\n",
    "            .transform('min')\n",
    "            .dt.year + avg_timespan\n",
    "        )\n",
    "        ret.loc[valid_rows, 'Age'] = ret.loc[valid_rows, 'Age'].fillna(ret.loc[valid_rows, 'predicted_age'])\n",
    "\n",
    "    ret.loc[~valid_rows, 'Age'] = np.nan\n",
    "\n",
    "    ret = ret.drop(columns=['predicted_age'], errors='ignore')\n",
    "\n",
    "    missing = ret.isnull().sum()\n",
    "    missing_percentage = missing / len(ret) * 100\n",
    "\n",
    "    for col in _columns_to_impute:\n",
    "        if ret[col].dtype in ['float64', 'int64']:\n",
    "            ret[col] = ret[col].fillna(ret[col].median())\n",
    "\n",
    "    ret = ret.drop(columns=_columns_to_drop)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_scaler(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_scaler(df:pd.DataFrame, train: bool) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    numerical_cols = [col for col in df.columns if ret[col].dtype in ['float64', 'int64']]\n",
    "    print(numerical_cols)\n",
    "    if train:\n",
    "        scaler.fit(ret[numerical_cols])\n",
    "    ret[numerical_cols] = scaler.transform(ret[numerical_cols])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aply_reduce_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_reduce_dim(df: pd.DataFrame, dim: int, epochs: int = 25) -> tuple:\n",
    "    ret = df.copy()\n",
    "    numerical_cols = [col for col in ret.columns if ret[col].dtype in ['float64', 'int64']]\n",
    "    cols_to_reduce = [col for col in numerical_cols if col != 'Age']\n",
    "    X = ret[cols_to_reduce]\n",
    "    input_dim = X.shape[1]\n",
    "\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(dim, activation='relu')(input_layer)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    autoencoder.fit(X, X, epochs=epochs, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "\n",
    "    encoder_model = Model(inputs=input_layer, outputs=encoder)\n",
    "    X_reduced = encoder_model.predict(X)\n",
    "    X_reduced = pd.DataFrame(X_reduced, columns=[f'feature_{i}' for i in range(dim)])\n",
    "    ret.drop(cols_to_reduce, axis=1, inplace=True)\n",
    "    ret = pd.concat([ret, X_reduced], axis=1)\n",
    "    return ret, encoder_model, cols_to_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_reduce_dim(df, encoder_model, cols_to_reduce) -> pd.DataFrame:\n",
    "    X = df[cols_to_reduce].copy()\n",
    "    for col in X.columns:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "    X_reduced = encoder_model.predict(X)\n",
    "    dim = X_reduced.shape[1]\n",
    "    X_reduced = pd.DataFrame(X_reduced, columns=[f'feature_{i}' for i in range(dim)], index=df.index)\n",
    "\n",
    "    df.drop(cols_to_reduce, axis=1, inplace=True)\n",
    "    df = pd.concat([df, X_reduced], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_time_encoding(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_time_encoding(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    ret['Last Maintenance'] = pd.to_datetime(ret['Last Maintenance'])\n",
    "    ret['timestamp'] = pd.to_datetime(ret['timestamp'])\n",
    "    ret['days_since_last_maintenance'] = (ret['timestamp'] - ret['Last Maintenance']).dt.days\n",
    "    ret.drop(['Last Maintenance', 'timestamp'], axis=1, inplace=True)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_one_hot_encode(df, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_one_hot_encode(df: pd.DataFrame, cols):\n",
    "    if df[cols].isnull().any():\n",
    "        df[cols] = df[cols].fillna(\"Missing\")\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    onehot_encoded = encoder.fit_transform(df[[cols]])\n",
    "    encoded_df = pd.DataFrame(onehot_encoded, columns=encoder.get_feature_names_out([cols]), index = df.index)\n",
    "    for column in encoded_df.columns:\n",
    "        encoded_df[column] = pd.Categorical(encoded_df[column])\n",
    "    df_encoded = pd.concat([df.drop(columns=cols), encoded_df], axis=1)\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_feature_selection(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_feature_selection(df: pd.DataFrame):\n",
    "    ret = df.copy()\n",
    "    # ret = ret.drop(columns=[\n",
    "    #     'ID_Area', 'ID_Mesin_x', 'Machine', 'Breakdown Category', 'Area', 'ID_Transaction',\n",
    "    #     'ID_Mesin_y', 'Mesin_x', 'Mesin_y', 'Country Machine_x', 'Country Machine_y'], errors='ignore')\n",
    "    # if 'Breakdown Category' in ret.columns:\n",
    "    #     ret = ret.drop(columns=['Breakdown Category'])\n",
    "\n",
    "    ret = ret[['feature_0', 'feature_1', 'feature_2', 'days_since_last_maintenance',\n",
    "       'Priority_High', 'Priority_Low', 'Priority_Medium', 'Priority_Missing',\n",
    "       'Status Sparepart_Broken', 'Status Sparepart_Empty',\n",
    "       'Status Sparepart_In Use', 'Status Sparepart_Missing',\n",
    "       'Status Sparepart_On Check', 'Status Sparepart_Ready',\n",
    "       'Status Sparepart_Repair', 'Power_Backup_Missing', 'Power_Backup_No',\n",
    "       'Power_Backup_Yes']]\n",
    "    \n",
    "    if \"Status\" in df.columns:\n",
    "        ret['Status'] = df['Status']\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_fill_category(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fill_category(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    for col in ret.select_dtypes(include=['category']).columns:\n",
    "            ret[col] = ret[col].cat.add_categories([-1])\n",
    "            ret[col] = ret[col].fillna(-1)\n",
    "    ret = ret.fillna(-1)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_smote(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smote(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    target = ret['Status'].map({'Normal': 0, 'Warning': 1, 'Breakdown': 2})\n",
    "    ret = ret.drop('Status', axis=1)\n",
    "    smote = SMOTE(random_state=42, n_jobs=-1)\n",
    "    X_resampled, y_resampled = smote.fit_resample(ret, target)\n",
    "    X_resampled = pd.DataFrame(X_resampled, columns=ret.columns)\n",
    "    X_resampled['Status'] = y_resampled\n",
    "    return X_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [78], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset/train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500_000\u001b[39m):\n\u001b[0;32m     11\u001b[0m     df \u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 12\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mapply_combine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     df \u001b[38;5;241m=\u001b[39m apply_fill_na(df)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m first_run:\n",
      "Cell \u001b[1;32mIn [66], line 17\u001b[0m, in \u001b[0;36mapply_combine\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     15\u001b[0m ret \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(ret, area_list_df, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID_Area\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m ret \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(ret, machine_list_df, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMachine\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID_Mesin\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmachine_area_filtered_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMachine\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mID_Area\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mID_Mesin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mID_Area\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\Adli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:152\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    151\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m--> 152\u001b[0m     left_df \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_operand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m     right_df \u001b[38;5;241m=\u001b[39m _validate_operand(right)\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Adli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:2684\u001b[0m, in \u001b[0;36m_validate_operand\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   2680\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_any\u001b[39m(x) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m   2681\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m com\u001b[38;5;241m.\u001b[39many_not_none(\u001b[38;5;241m*\u001b[39mx)\n\u001b[1;32m-> 2684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_operand\u001b[39m(obj: DataFrame \u001b[38;5;241m|\u001b[39m Series) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   2685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, ABCDataFrame):\n\u001b[0;32m   2686\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "first_run = False\n",
    "idx = 0\n",
    "cols_to_reduce = ['temperature_10H_max (°C)',\n",
    " 'temperature-1',\n",
    " 'temperature-3',\n",
    " 'Voltage-M',\n",
    " 'Current-M',\n",
    " 'Current-R']\n",
    "\n",
    "for chunk in pd.read_csv(\"dataset/train.csv\", chunksize=500_000):\n",
    "    df = chunk.copy()\n",
    "    df = apply_combine(df)\n",
    "    df = apply_fill_na(df)\n",
    "\n",
    "    if first_run:\n",
    "        df, encoder_model, cols_to_reduce = apply_reduce_dim(df, 3)\n",
    "        encoder_model.save(\"saved_models/encoder.keras\")\n",
    "        first_run = False\n",
    "    else:\n",
    "        encoder_model = load_model(\"saved_models/encoder.keras\")\n",
    "        df = predict_reduce_dim(df, encoder_model, cols_to_reduce)\n",
    "    \n",
    "    df = apply_time_encoding(df)\n",
    "    df = apply_one_hot_encode(df, 'Priority')\n",
    "    df = apply_one_hot_encode(df, 'Status Sparepart')\n",
    "    df = apply_one_hot_encode(df, 'Power_Backup')\n",
    "    df = apply_fill_category(df)\n",
    "    df = apply_feature_selection(df)\n",
    "    df = apply_scaler(df, train=True)\n",
    "    df = apply_smote(df)\n",
    "\n",
    "    X = df.drop(columns='Status')\n",
    "    y = df['Status']\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=25, max_depth=4, n_jobs=-1, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    pickle.dump(rf, open(f\"saved_models/rf_{idx}.pkl\", 'wb'))\n",
    "\n",
    "    idx +=1\n",
    "    print(idx)\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PREDICTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(id: pd.Series, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(26):\n",
    "        rf = pickle.load(open(f\"saved_models/rf_{i}.pkl\", 'rb'))\n",
    "        \n",
    "        y_pred = rf.predict_proba(df)\n",
    "        \n",
    "        max_indices = y_pred.argmax(axis=1)\n",
    "        \n",
    "        predictions.append(max_indices)\n",
    "    \n",
    "    majority_votes = pd.DataFrame(predictions).mode(axis=0).iloc[0]\n",
    "    \n",
    "    result = pd.DataFrame({\n",
    "        'ID': id,\n",
    "        'Prediction': majority_votes\n",
    "    })\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "['days_since_last_maintenance']\n"
     ]
    }
   ],
   "source": [
    "pred_result = []\n",
    "for chunk in pd.read_csv(\"dataset/test.csv\", chunksize=100_000):\n",
    "    df = chunk.copy()\n",
    "    ID = df['ID_Transaction'].copy()\n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    df = apply_combine(df)\n",
    "    df = apply_fill_na_predict(df)\n",
    "\n",
    "    df = predict_reduce_dim(df, encoder_model, cols_to_reduce)\n",
    "    \n",
    "    df = apply_time_encoding(df)\n",
    "    df = apply_one_hot_encode(df, 'Priority')\n",
    "    df = apply_one_hot_encode(df, 'Status Sparepart')\n",
    "    df = apply_one_hot_encode(df, 'Power_Backup')\n",
    "    df = apply_feature_selection(df)\n",
    "    df = apply_fill_category(df)\n",
    "    df = apply_scaler(df, train=False)\n",
    "\n",
    "    y_pred = predict(df)\n",
    "\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
