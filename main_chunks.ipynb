{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers # type: ignore\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization # type: ignore\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau # type: ignore\n",
    "from tensorflow.keras.models import load_model # type: ignore\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Model # type: ignore\n",
    "from tensorflow.keras.utils import to_categorical # type: ignore\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FUNCTION DECLARATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_combine(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_area_df = pd.read_csv(\"dataset/Machine-Area.csv\")\n",
    "machine_list_df = pd.read_csv(\"dataset/Machine-List.csv\")\n",
    "area_list_df = pd.read_csv(\"dataset/Area-List.csv\")\n",
    "machine_area_df['Last Maintenance'] = pd.to_datetime(machine_area_df['Last Maintenance'])\n",
    "maintenance_frequency = machine_area_df.groupby('ID_Mesin').size().rename('maintenance_count')\n",
    "machine_area_df = pd.merge(machine_area_df, maintenance_frequency, left_on='ID_Mesin', right_index=True, how='left')\n",
    "machine_area_filtered_df = machine_area_df.sort_values(\n",
    "    by=['ID_Area', 'ID_Mesin', 'Last Maintenance'], \n",
    "    ascending=[True, True, False]\n",
    ").drop_duplicates(subset=['ID_Area', 'ID_Mesin'], keep='first')\n",
    "\n",
    "def apply_combine(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret: pd.DataFrame = df.copy()\n",
    "    \n",
    "    ret = pd.merge(ret, area_list_df, on='ID_Area', how='left')\n",
    "    ret = pd.merge(ret, machine_list_df, left_on='Machine', right_on='ID_Mesin', how='left')\n",
    "    ret = pd.merge(ret, machine_area_filtered_df, left_on=['Machine', 'ID_Area'], right_on=['ID_Mesin', 'ID_Area'], how='left')\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_fill_na(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "_columns_to_keep = ['Last Maintenance', 'Status Sparepart', 'Power_Backup', 'Priority', 'Status', 'Age', 'maintenance_count', 'Country Machine_x']\n",
    "\n",
    "_columns_to_impute: list[str] = []\n",
    "_columns_to_drop: list[str] = []\n",
    "\n",
    "def apply_fill_na(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "\n",
    "    avg_timespan = 10\n",
    "    valid_rows = ret['ID_Mesin_x'].notnull() & ret['ID_Area'].notnull()\n",
    "\n",
    "    if valid_rows.any():\n",
    "        ret.loc[valid_rows, 'predicted_age'] = (\n",
    "            ret.loc[valid_rows]\n",
    "            .groupby(['ID_Mesin_x', 'ID_Area'])['Last Maintenance']\n",
    "            .transform('min')\n",
    "            .dt.year + avg_timespan\n",
    "        )\n",
    "        ret.loc[valid_rows, 'Age'] = ret.loc[valid_rows, 'Age'].fillna(ret.loc[valid_rows, 'predicted_age'])\n",
    "\n",
    "    ret.loc[~valid_rows, 'Age'] = np.nan\n",
    "\n",
    "    ret = ret.drop(columns=['predicted_age'], errors='ignore')\n",
    "\n",
    "    missing = ret.isnull().sum()\n",
    "    missing_percentage = missing / len(ret) * 100\n",
    "\n",
    "    columns_to_impute = missing_percentage[missing_percentage < 5].index\n",
    "    for col in columns_to_impute:\n",
    "        if ret[col].dtype in ['float64', 'int64']:\n",
    "            ret[col] = ret[col].fillna(ret[col].median())\n",
    "\n",
    "    missing_percentage = ret.isnull().sum() / len(ret) * 100\n",
    "    columns_to_drop = missing_percentage[missing_percentage > 5].index\n",
    "    filtered_columns_to_drop = [col for col in columns_to_drop if col not in _columns_to_keep]\n",
    "    ret = ret.drop(columns=filtered_columns_to_drop)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fill_na_predict(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "\n",
    "    avg_timespan = 10\n",
    "    valid_rows = ret['ID_Mesin_x'].notnull() & ret['ID_Area'].notnull()\n",
    "\n",
    "    if valid_rows.any():\n",
    "        ret.loc[valid_rows, 'predicted_age'] = (\n",
    "            ret.loc[valid_rows]\n",
    "            .groupby(['ID_Mesin_x', 'ID_Area'])['Last Maintenance']\n",
    "            .transform('min')\n",
    "            .dt.year + avg_timespan\n",
    "        )\n",
    "        ret.loc[valid_rows, 'Age'] = ret.loc[valid_rows, 'Age'].fillna(ret.loc[valid_rows, 'predicted_age'])\n",
    "\n",
    "    ret.loc[~valid_rows, 'Age'] = np.nan\n",
    "\n",
    "    ret = ret.drop(columns=['predicted_age'], errors='ignore')\n",
    "\n",
    "    missing = ret.isnull().sum()\n",
    "    missing_percentage = missing / len(ret) * 100\n",
    "\n",
    "    for col in _columns_to_impute:\n",
    "        if ret[col].dtype in ['float64', 'int64']:\n",
    "            ret[col] = ret[col].fillna(ret[col].median())\n",
    "\n",
    "    ret = ret.drop(columns=_columns_to_drop)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_scaler(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_scaler(df:pd.DataFrame, train: bool) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    numerical_cols = ['temperature_10H_max (°C)', 'temperature-1', 'temperature-3',\n",
    "       'Voltage-M', 'Current-M', 'Current-R']\n",
    "    if train:\n",
    "        scaler.fit(ret[numerical_cols])\n",
    "    ret[numerical_cols] = scaler.transform(ret[numerical_cols])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aply_reduce_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_reduce_dim(df, dim):\n",
    "    cols_to_reduce = ['temperature_10H_max (°C)',\n",
    "                      'temperature-1', 'temperature-3',\n",
    "                      'Voltage-M', 'Current-M', 'Current-R']\n",
    "    X = df[cols_to_reduce]\n",
    "    input_dim = X.shape[1]\n",
    "\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(dim, activation='relu')(input_layer)\n",
    "    decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    autoencoder.fit(X, X, epochs=22, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "\n",
    "    encoder_model = Model(inputs=input_layer, outputs=encoder)\n",
    "    X_reduced = encoder_model.predict(X)\n",
    "    X_reduced = pd.DataFrame(X_reduced, columns=[f'feature_{i}' for i in range(dim)], index=df.index)\n",
    "    df.drop(cols_to_reduce, axis=1, inplace=True)\n",
    "    df = pd.concat([df, X_reduced], axis=1)\n",
    "    return df, encoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_reduce_dim(df, encoder_model) :\n",
    "    cols_to_reduce = ['temperature_10H_max (°C)',\n",
    "                      'temperature-1', 'temperature-3',\n",
    "                      'Voltage-M', 'Current-M', 'Current-R']\n",
    "    X = df[cols_to_reduce].copy()\n",
    "    for col in X.columns:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "    X_reduced = encoder_model.predict(X)\n",
    "    dim = X_reduced.shape[1]\n",
    "    X_reduced = pd.DataFrame(X_reduced, columns=[f'feature_{i}' for i in range(dim)], index=df.index)\n",
    "\n",
    "    df.drop(cols_to_reduce, axis=1, inplace=True)\n",
    "    df = pd.concat([df, X_reduced], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_time_encoding(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_time_encoding(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    ret['Last Maintenance'] = pd.to_datetime(ret['Last Maintenance'])\n",
    "    ret['timestamp'] = pd.to_datetime(ret['timestamp'])\n",
    "    ret['days_since_last_maintenance'] = (ret['timestamp'] - ret['Last Maintenance']).dt.days\n",
    "    ret.drop(['Last Maintenance', 'timestamp'], axis=1, inplace=True)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_one_hot_encode(df, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_one_hot_encode(df: pd.DataFrame, cols):\n",
    "    if df[cols].isnull().any():\n",
    "        df[cols] = df[cols].fillna(\"Missing\")\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    onehot_encoded = encoder.fit_transform(df[[cols]])\n",
    "    encoded_df = pd.DataFrame(onehot_encoded, columns=encoder.get_feature_names_out([cols]), index = df.index)\n",
    "    for column in encoded_df.columns:\n",
    "        encoded_df[column] = pd.Categorical(encoded_df[column])\n",
    "    df_encoded = pd.concat([df.drop(columns=cols), encoded_df], axis=1)\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_feature_selection(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_feature_selection(df: pd.DataFrame):\n",
    "    ret = df.copy()\n",
    "    # ret = ret.drop(columns=[\n",
    "    #     'ID_Area', 'ID_Mesin_x', 'Machine', 'Breakdown Category', 'Area', 'ID_Transaction',\n",
    "    #     'ID_Mesin_y', 'Mesin_x', 'Mesin_y', 'Country Machine_x', 'Country Machine_y'], errors='ignore')\n",
    "    # if 'Breakdown Category' in ret.columns:\n",
    "    #     ret = ret.drop(columns=['Breakdown Category'])\n",
    "\n",
    "    ret = ret[['feature_0', 'feature_1', 'days_since_last_maintenance',\n",
    "       'Priority_High', 'Priority_Low', 'Priority_Medium',\n",
    "       'Status Sparepart_Broken', 'Status Sparepart_Empty',\n",
    "       'Status Sparepart_In Use',\n",
    "       'Status Sparepart_On Check', 'Status Sparepart_Ready',\n",
    "       'Status Sparepart_Repair', 'Power_Backup_No',\n",
    "       'Power_Backup_Yes']]\n",
    "    \n",
    "    if \"Status\" in df.columns:\n",
    "        ret['Status'] = df['Status']\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_fill_category(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fill_category(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    for col in ret.select_dtypes(include=['category']).columns:\n",
    "            ret[col] = ret[col].cat.add_categories([-1])\n",
    "            ret[col] = ret[col].fillna(-1)\n",
    "    ret = ret.fillna(-1)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_smote(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smote(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    target = ret['Status'].map({'Normal': 0, 'Warning': 1, 'Breakdown': 2})\n",
    "    ret = ret.drop('Status', axis=1)\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(ret, target)\n",
    "    X_resampled = pd.DataFrame(X_resampled, columns=ret.columns)\n",
    "    X_resampled['Status'] = y_resampled\n",
    "    return X_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9375/9375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 826us/step\n",
      "1\n",
      "\u001b[1m9375/9375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 669us/step\n",
      "2\n",
      "\u001b[1m9375/9375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 771us/step\n",
      "3\n",
      "\u001b[1m9375/9375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 647us/step\n",
      "4\n",
      "\u001b[1m9375/9375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 671us/step\n",
      "5\n",
      "\u001b[1m9375/9375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 834us/step\n",
      "6\n",
      "\u001b[1m9375/9375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 855us/step\n",
      "7\n",
      "\u001b[1m9375/9375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 682us/step\n",
      "8\n",
      "\u001b[1m9375/9375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 763us/step\n",
      "9\n",
      "\u001b[1m9375/9375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 689us/step\n",
      "10\n",
      "\u001b[1m9375/9375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 771us/step\n",
      "11\n",
      "\u001b[1m9375/9375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 675us/step\n",
      "12\n",
      "\u001b[1m9375/9375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 773us/step\n",
      "13\n",
      "\u001b[1m9375/9375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 753us/step\n",
      "14\n",
      "\u001b[1m9375/9375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 763us/step\n",
      "15\n",
      "\u001b[1m9375/9375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 657us/step\n",
      "16\n",
      "\u001b[1m9375/9375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step\n",
      "17\n",
      "\u001b[1m9375/9375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 670us/step\n",
      "18\n",
      "\u001b[1m9375/9375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 646us/step\n",
      "19\n",
      "\u001b[1m9375/9375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 725us/step\n",
      "20\n",
      "\u001b[1m9375/9375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 667us/step\n",
      "21\n",
      "\u001b[1m9375/9375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 666us/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [205], line 37\u001b[0m\n\u001b[0;32m     33\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     35\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(rf, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_models/rf_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Adli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Adli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\Adli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Adli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Adli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Adli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "first_run = False\n",
    "idx = 0\n",
    "cols_to_reduce = ['temperature_10H_max (°C)',\n",
    " 'temperature-1',\n",
    " 'temperature-3',\n",
    " 'Voltage-M',\n",
    " 'Current-M',\n",
    " 'Current-R']\n",
    "\n",
    "for chunk in pd.read_csv(\"dataset/train.csv\", chunksize=300_000):\n",
    "    df = chunk.copy()\n",
    "    df = apply_combine(df)\n",
    "    df = apply_fill_na(df)\n",
    "\n",
    "    df = apply_scaler(df, train=True)\n",
    "    if first_run:\n",
    "        df, encoder_model = apply_reduce_dim(df, 3)\n",
    "        encoder_model.save(\"saved_models/encoder.keras\")\n",
    "        first_run = False\n",
    "    else:\n",
    "        encoder_model = load_model(\"saved_models/encoder.keras\")\n",
    "        df = predict_reduce_dim(df, encoder_model)\n",
    "    \n",
    "    df = apply_time_encoding(df)\n",
    "    df = apply_one_hot_encode(df, 'Priority')\n",
    "    df = apply_one_hot_encode(df, 'Status Sparepart')\n",
    "    df = apply_one_hot_encode(df, 'Power_Backup')\n",
    "    df = apply_fill_category(df)\n",
    "    df = apply_feature_selection(df)\n",
    "    df = apply_smote(df)\n",
    "\n",
    "    X = df.drop(columns='Status')\n",
    "    y = df['Status']\n",
    "\n",
    "    rf = RandomForestClassifier(n_jobs=-1, random_state=42, n_estimators=50, max_depth=15)\n",
    "\n",
    "    rf.fit(X, y)\n",
    "    joblib.dump(rf, f\"saved_models/rf_{idx}.pkl\")\n",
    "\n",
    "    idx +=1\n",
    "    print(idx)\n",
    "\n",
    "    del rf\n",
    "    del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PREDICTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(id: pd.Series, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    summed_probabilities = None\n",
    "    final_predictions = pd.Series()\n",
    "    \n",
    "    for i in range(21):\n",
    "        rf = joblib.load(f\"saved_models/rf_{i}.pkl\")\n",
    "        \n",
    "        y_pred = rf.predict_proba(df)\n",
    "        \n",
    "        if summed_probabilities is None:\n",
    "            summed_probabilities = y_pred\n",
    "        else:\n",
    "            summed_probabilities += y_pred\n",
    "    \n",
    "    final_predictions = summed_probabilities.argmax(axis=1)\n",
    "    \n",
    "    result = pd.DataFrame({\n",
    "        'ID': id,\n",
    "        'Status': final_predictions\n",
    "    })\n",
    "\n",
    "    result['Status'] = result['Status'].map({0: 'Normal', 1: 'Warning', 2: 'Breakdown'})\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_medok(id: pd.Series, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    all_predictions = []\n",
    "\n",
    "    for i in range(21):\n",
    "        rf = joblib.load(f\"saved_models/rf_{i}.pkl\")\n",
    "        \n",
    "        y_pred = rf.predict(df)\n",
    "        \n",
    "        all_predictions.append(y_pred)\n",
    "    \n",
    "    all_predictions_df = pd.DataFrame(all_predictions).T\n",
    "    \n",
    "    final_predictions = all_predictions_df.mode(axis=1)[0]\n",
    "    \n",
    "    result = pd.DataFrame({\n",
    "        'ID_Transaction': id,\n",
    "        'Status': final_predictions\n",
    "    })\n",
    "\n",
    "    result['Status'] = result['Status'].map({0: 'Normal', 1: 'Warning', 2: 'Breakdown'})\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 837us/step\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [230], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m df \u001b[38;5;241m=\u001b[39m apply_feature_selection(df)\n\u001b[0;32m     20\u001b[0m df \u001b[38;5;241m=\u001b[39m apply_fill_category(df)\n\u001b[1;32m---> 22\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m submission \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([submission, y_pred])\n",
      "Cell \u001b[1;32mIn [225], line 15\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(id, df)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m         summed_probabilities \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m y_pred\n\u001b[1;32m---> 15\u001b[0m final_predictions \u001b[38;5;241m=\u001b[39m \u001b[43msummed_probabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m result \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mid\u001b[39m,\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m'\u001b[39m: final_predictions\n\u001b[0;32m     20\u001b[0m })\n\u001b[0;32m     22\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap({\u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNormal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWarning\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBreakdown\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "pred_result = []\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "for chunk in pd.read_csv(\"dataset/test.csv\", chunksize=100_000):\n",
    "    df = chunk.copy()\n",
    "    ID = df['ID_Transaction'].copy()\n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    df = apply_combine(df)\n",
    "    df = apply_fill_na_predict(df)\n",
    "\n",
    "    df = apply_scaler(df, train=False)\n",
    "    encoder_model = load_model(\"saved_models/encoder.keras\")\n",
    "    df = predict_reduce_dim(df, encoder_model)\n",
    "    \n",
    "    df = apply_time_encoding(df)\n",
    "    df = apply_one_hot_encode(df, 'Priority')\n",
    "    df = apply_one_hot_encode(df, 'Status Sparepart')\n",
    "    df = apply_one_hot_encode(df, 'Power_Backup')\n",
    "    df = apply_feature_selection(df)\n",
    "    df = apply_fill_category(df)\n",
    "\n",
    "    y_pred = predict_medok(ID, df)\n",
    "\n",
    "    submission = pd.concat([submission, y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status\n",
       "Normal       6984085\n",
       "Warning        11351\n",
       "Breakdown       4564\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['Status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.columns = ['ID_Transaction', 'Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Transaction</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRXb7e33ef41eea9cfdc0d1c338bad7f0d4</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRX8fc5889e25fbf66b21063d165228745f</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRX5c56664724a974cf6c87bd2659fd7046</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRX85fe42a5737897b7649a24fede27e90f</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRX16fae83f3c8c66fb15f088e7da7713ee</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999995</th>\n",
       "      <td>TRX853c4fa3ea09c9872e544ad425144181</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999996</th>\n",
       "      <td>TRXec71df6ec2cc1ad5b69d08fdd8d3aea9</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999997</th>\n",
       "      <td>TRX2cf42b2824f7d87d85549ddbb81d4e51</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999998</th>\n",
       "      <td>TRX4bc41281d855f597df32e02f0d4e8fc1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999999</th>\n",
       "      <td>TRX01d39692f20956493ee81974c4b3e21b</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              ID_Transaction  Status\n",
       "0        TRXb7e33ef41eea9cfdc0d1c338bad7f0d4  Normal\n",
       "1        TRX8fc5889e25fbf66b21063d165228745f  Normal\n",
       "2        TRX5c56664724a974cf6c87bd2659fd7046  Normal\n",
       "3        TRX85fe42a5737897b7649a24fede27e90f  Normal\n",
       "4        TRX16fae83f3c8c66fb15f088e7da7713ee  Normal\n",
       "...                                      ...     ...\n",
       "6999995  TRX853c4fa3ea09c9872e544ad425144181  Normal\n",
       "6999996  TRXec71df6ec2cc1ad5b69d08fdd8d3aea9  Normal\n",
       "6999997  TRX2cf42b2824f7d87d85549ddbb81d4e51  Normal\n",
       "6999998  TRX4bc41281d855f597df32e02f0d4e8fc1  Normal\n",
       "6999999  TRX01d39692f20956493ee81974c4b3e21b  Normal\n",
       "\n",
       "[7000000 rows x 2 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TESTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "farrell = pd.read_csv(\"farrell.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID_Transaction', 'Status'], dtype='object')"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "farrell.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15625/15625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# first_run = False\n",
    "# idx = 0\n",
    "# cols_to_reduce = ['temperature_10H_max (°C)',\n",
    "#  'temperature-1',\n",
    "#  'temperature-3',\n",
    "#  'Voltage-M',\n",
    "#  'Current-M',\n",
    "#  'Current-R']\n",
    "\n",
    "# for chunk in pd.read_csv(\"dataset/train.csv\", chunksize=500_000):\n",
    "#     df = chunk.copy()\n",
    "#     df = apply_combine(df)\n",
    "#     df = apply_fill_na(df)\n",
    "\n",
    "#     df = apply_scaler(df, train=True)\n",
    "#     if first_run:\n",
    "#         df, encoder_model = apply_reduce_dim(df, 2)\n",
    "#         encoder_model.save(\"saved_models/encoder_2.keras\")\n",
    "#         first_run = False\n",
    "#     else:\n",
    "#         encoder_model = load_model(\"saved_models/encoder.keras\")\n",
    "#         df = predict_reduce_dim(df, encoder_model)\n",
    "    \n",
    "#     df = apply_time_encoding(df)\n",
    "#     df = apply_one_hot_encode(df, 'Priority')\n",
    "#     df = apply_one_hot_encode(df, 'Status Sparepart')\n",
    "#     df = apply_one_hot_encode(df, 'Power_Backup')\n",
    "#     df = apply_fill_category(df)\n",
    "#     df = apply_feature_selection(df)\n",
    "#     df = apply_smote(df)\n",
    "\n",
    "#     X = df.drop(columns='Status')\n",
    "#     y = df['Status']\n",
    "\n",
    "#     rf = RandomForestClassifier(n_jobs=-1, random_state=42, n_estimators=50, max_depth=15)\n",
    "#     rf.fit(X, y)\n",
    "\n",
    "#     break\n",
    "\n",
    "# del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2770/9375\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 909us/step"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [229], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m df \u001b[38;5;241m=\u001b[39m apply_scaler(df, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m encoder_model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_models/encoder.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_reduce_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m df \u001b[38;5;241m=\u001b[39m apply_time_encoding(df)\n\u001b[0;32m     16\u001b[0m df \u001b[38;5;241m=\u001b[39m apply_one_hot_encode(df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPriority\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn [8], line 8\u001b[0m, in \u001b[0;36mpredict_reduce_dim\u001b[1;34m(df, encoder_model)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m X\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m      7\u001b[0m     X[col] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(X[col], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m X_reduced \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m dim \u001b[38;5;241m=\u001b[39m X_reduced\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     10\u001b[0m X_reduced \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_reduced, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(dim)], index\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Adli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Adli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:509\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    508\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m--> 509\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m     batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_function(data)\n\u001b[0;32m    511\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m append_to_outputs(batch_outputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\Adli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:489\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict.<locals>.get_data\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution):\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 489\u001b[0m         single_step_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mStopIteration\u001b[39;00m, tf\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mOutOfRangeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    491\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__len__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    492\u001b[0m             \u001b[38;5;66;03m# Suppress the error when still have remaining data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Adli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:826\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    825\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    827\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Adli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:776\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[1;32m--> 776\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Adli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3108\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3107\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3108\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3109\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIteratorGetNext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3110\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_shapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3112\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "skip = 0\n",
    "for chunk in pd.read_csv(\"dataset/train.csv\", chunksize=300_000):\n",
    "    if skip == 5:\n",
    "        skip += 1\n",
    "        continue\n",
    "    df = chunk.copy()\n",
    "    df = apply_combine(df)\n",
    "    df = apply_fill_na_predict(df)\n",
    "\n",
    "    df = apply_scaler(df, train=True)\n",
    "    \n",
    "    encoder_model = load_model(\"saved_models/encoder.keras\")\n",
    "    df = predict_reduce_dim(df, encoder_model)\n",
    "    \n",
    "    df = apply_time_encoding(df)\n",
    "    df = apply_one_hot_encode(df, 'Priority')\n",
    "    df = apply_one_hot_encode(df, 'Status Sparepart')\n",
    "    df = apply_one_hot_encode(df, 'Power_Backup')\n",
    "    df = apply_fill_category(df)\n",
    "    df = apply_feature_selection(df)\n",
    "\n",
    "    X = df.drop(columns='Status')\n",
    "    y = df['Status']\n",
    "    \n",
    "    y_pred = rf.predict(X)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.4846896103436038\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# # Calculate F1 score\n",
    "# _y = y.map({'Normal': 0, 'Warning': 1, 'Breakdown': 2})\n",
    "# f1 = f1_score(_y, y_pred, average='weighted') \n",
    "# # \n",
    "# print(\"F1 Score:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
