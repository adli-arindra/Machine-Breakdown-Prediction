{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Importing Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset(path: str, nrows: int | None = None) -> pd.DataFrame:\n",
    "    chunksize = 100_000\n",
    "    machine_area_df = pd.read_csv(\"dataset/Machine-Area.csv\")\n",
    "    machine_list_df = pd.read_csv(\"dataset/Machine-List.csv\")\n",
    "    area_list_df = pd.read_csv(\"dataset/Area-List.csv\")\n",
    "    machine_area_df['Last Maintenance'] = pd.to_datetime(machine_area_df['Last Maintenance'])\n",
    "    machine_area_filtered_df = machine_area_df.sort_values(\n",
    "        by=['ID_Area', 'ID_Mesin', 'Last Maintenance'], \n",
    "        ascending=[True, True, False]\n",
    "    ).drop_duplicates(subset=['ID_Area', 'ID_Mesin'], keep='first')\n",
    "\n",
    "    id_list = []\n",
    "    chunk_list = []\n",
    "    for chunk in pd.read_csv(path, chunksize=chunksize, nrows=nrows, low_memory=False):\n",
    "        id_column = chunk.pop('ID_Transaction') if 'ID_Transaction' in chunk.columns else None\n",
    "        chunk = pd.merge(chunk, area_list_df, on='ID_Area', how='left')\n",
    "        chunk = pd.merge(chunk, machine_list_df, left_on='Machine', right_on='ID_Mesin', how='left')\n",
    "        chunk = pd.merge(chunk, machine_area_filtered_df, left_on=['Machine', 'ID_Area'], right_on=['ID_Mesin', 'ID_Area'], how='left')\n",
    "\n",
    "        chunk_list.append(chunk)\n",
    "        if id_column is not None:\n",
    "            id_list.append(id_column)\n",
    "    combined_df = pd.concat(chunk_list, ignore_index=True)\n",
    "    id_series = pd.concat(id_list, ignore_index=True) if id_list else None\n",
    "    return combined_df, id_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, _ = import_dataset(\n",
    "    path=\"dataset/train.csv\", \n",
    "    nrows=200_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df, test_id = import_dataset(\n",
    "    path=\"dataset/test.csv\", \n",
    "    nrows=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature_10H_max (°C)     2.8675\n",
      "temperature_10H_min (°C)    19.1205\n",
      "temperature-1                2.9615\n",
      "temperature-2               11.4110\n",
      "temperature-3                3.5470\n",
      "apparent_temperature_max    21.2770\n",
      "apparent_temperature_min    20.1760\n",
      "ID_Area                     17.3180\n",
      "Machine                     16.8720\n",
      "timestamp                    0.0000\n",
      "humidity                    19.4545\n",
      "Voltage-L                    9.7585\n",
      "Voltage-R                    6.8485\n",
      "Voltage-M                    3.4415\n",
      "Current-M                    1.0965\n",
      "Current-R                    1.3795\n",
      "Current-T                   22.6750\n",
      "RPM                         11.0755\n",
      "RPM-1                       13.3295\n",
      "RPM-2                       18.6955\n",
      "RPM-3                       18.3175\n",
      "Vibration-1                  5.3790\n",
      "Vibration-2                 15.0745\n",
      "Power                       12.9790\n",
      "Power_Backup                 3.3195\n",
      "Status                       0.0000\n",
      "Breakdown Category          73.7400\n",
      "Area                        17.3180\n",
      "Priority                    17.3180\n",
      "ID_Mesin_x                  16.8720\n",
      "Mesin_x                     16.8720\n",
      "Country Machine_x           16.8720\n",
      "ID_Mesin_y                  39.5765\n",
      "Mesin_y                     39.5765\n",
      "Country Machine_y           39.5765\n",
      "Last Maintenance            39.5765\n",
      "Status Sparepart            39.5765\n",
      "Age                         39.5765\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "missing = train_df.isnull().sum()\n",
    "missing_percentage = missing / len(train_df) * 100\n",
    "print(missing_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_to_impute = missing_percentage[missing_percentage < 5].index\n",
    "for col in columns_to_impute:\n",
    "    if train_df[col].dtype in ['float64', 'int64']:\n",
    "        train_df[col] = train_df[col].fillna(train_df[col].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample_impute(series):\n",
    "    non_missing = series.dropna()\n",
    "    return series.apply(lambda x: np.random.choice(non_missing) if pd.isnull(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "def knn_impute_single_column(series, n_neighbors=2):\n",
    "    series_df = series.to_frame()\n",
    "    knn_imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    imputed_array = knn_imputer.fit_transform(series_df)\n",
    "    return pd.Series(imputed_array.ravel(), index=series.index, name=series.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed temperature_10H_min (°C)\n",
      "Imputed temperature-2\n",
      "Imputed apparent_temperature_max\n",
      "Imputed apparent_temperature_min\n",
      "Imputed humidity\n",
      "Imputed Voltage-L\n",
      "Imputed Voltage-R\n",
      "Imputed Current-T\n",
      "Imputed RPM\n",
      "Imputed RPM-1\n",
      "Imputed RPM-2\n",
      "Imputed RPM-3\n",
      "Imputed Vibration-1\n",
      "Imputed Vibration-2\n",
      "Imputed Power\n"
     ]
    }
   ],
   "source": [
    "moderate_missingness_cols = missing_percentage[(missing_percentage >= 5) & (missing_percentage <= 23)].index\n",
    "numerical_cols = [col for col in moderate_missingness_cols if train_df[col].dtype in ['float64', 'int64']]\n",
    "\n",
    "for col in numerical_cols:\n",
    "    train_df[col] = random_sample_impute(train_df[col])\n",
    "    print(f\"Imputed {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature_10H_max (°C)     0.0000\n",
      "temperature_10H_min (°C)     0.0000\n",
      "temperature-1                0.0000\n",
      "temperature-2                0.0000\n",
      "temperature-3                0.0000\n",
      "apparent_temperature_max     0.0000\n",
      "apparent_temperature_min     0.0000\n",
      "ID_Area                     17.3180\n",
      "Machine                     16.8720\n",
      "timestamp                    0.0000\n",
      "humidity                     0.0000\n",
      "Voltage-L                    0.0000\n",
      "Voltage-R                    0.0000\n",
      "Voltage-M                    0.0000\n",
      "Current-M                    0.0000\n",
      "Current-R                    0.0000\n",
      "Current-T                    0.0000\n",
      "RPM                          0.0000\n",
      "RPM-1                        0.0000\n",
      "RPM-2                        0.0000\n",
      "RPM-3                        0.0000\n",
      "Vibration-1                  0.0000\n",
      "Vibration-2                  0.0000\n",
      "Power                        0.0000\n",
      "Power_Backup                 3.3195\n",
      "Status                       0.0000\n",
      "Breakdown Category          73.7400\n",
      "Area                        17.3180\n",
      "Priority                    17.3180\n",
      "ID_Mesin_x                  16.8720\n",
      "Mesin_x                     16.8720\n",
      "Country Machine_x           16.8720\n",
      "ID_Mesin_y                  39.5765\n",
      "Mesin_y                     39.5765\n",
      "Country Machine_y           39.5765\n",
      "Last Maintenance            39.5765\n",
      "Status Sparepart            39.5765\n",
      "Age                         39.5765\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "missing = train_df.isnull().sum()\n",
    "missing_percentage = missing / len(train_df) * 100\n",
    "print(missing_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler(df):\n",
    "    numerical_cols = [col for col in df.columns if df[col].dtype in ['float64', 'int64']]\n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bisa pake feature hasher buat categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dim(df, dim):\n",
    "    numerical_cols = [col for col in df.columns if df[col].dtype in ['float64', 'int64']]\n",
    "    cols_to_reduce = [col for col in numerical_cols if col != 'Age']\n",
    "    X = df[cols_to_reduce]\n",
    "    input_dim = X.shape[1]\n",
    "\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(dim, activation='relu')(input_layer)\n",
    "    decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    autoencoder.fit(X, X, epochs=50, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "\n",
    "    encoder_model = Model(inputs=input_layer, outputs=encoder)\n",
    "    X_reduced = encoder_model.predict(X)\n",
    "    X_reduced = pd.DataFrame(X_reduced, columns=[f'feature_{i}' for i in range(dim)])\n",
    "    df.drop(cols_to_reduce, axis=1, inplace=True)\n",
    "    df = pd.concat([df, X_reduced], axis=1)\n",
    "    return df, encoder_model, cols_to_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_reduce_dim(df, encoder_model, cols_to_reduce) :\n",
    "    X = df[cols_to_reduce].copy()\n",
    "    for col in X.columns:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "    X_reduced = encoder_model.predict(X)\n",
    "    dim = X_reduced.shape[1]\n",
    "    X_reduced = pd.DataFrame(X_reduced, columns=[f'feature_{i}' for i in range(dim)], index=df.index)\n",
    "\n",
    "    df.drop(cols_to_reduce, axis=1, inplace=True)\n",
    "    df = pd.concat([df, X_reduced], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df, cols):\n",
    "    if df[cols].isnull().any():\n",
    "        df[cols] = df[cols].fillna(\"Missing\")\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    onehot_encoded = encoder.fit_transform(df[[cols]])\n",
    "    encoded_df = pd.DataFrame(onehot_encoded, columns=encoder.get_feature_names_out([cols]), index = df.index)\n",
    "    for column in encoded_df.columns:\n",
    "        encoded_df[column] = pd.Categorical(encoded_df[column])\n",
    "    df_encoded = pd.concat([df.drop(columns=cols), encoded_df], axis=1)\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_encoding(df):\n",
    "    df['Last Maintenance'] = pd.to_datetime(df['Last Maintenance'])\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['days_since_last_maintenance'] = (df['timestamp'] - df['Last Maintenance']).dt.days\n",
    "    df.drop(['Last Maintenance', 'timestamp'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Imbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_transform(df):\n",
    "    target = df['Status'].map({'Normal': 0, 'Warning': 1, 'Breakdown': 2})\n",
    "    df = df.drop('Status', axis=1)\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(df, target)\n",
    "    X_resampled = pd.DataFrame(X_resampled, columns=df.columns)\n",
    "    X_resampled['Status'] = y_resampled\n",
    "    return X_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(df, is_train=False, encoder_model=None, cols_to_reduce=None):\n",
    "    df = scaler(df)\n",
    "    if is_train:\n",
    "        df, encoder_model, cols_to_reduce = reduce_dim(df, 3)\n",
    "    else:\n",
    "        if encoder_model is None or cols_to_reduce is None:\n",
    "            raise ValueError(\"For non-training data, 'encoder_model' and 'cols_to_reduce' must be provided.\")\n",
    "        df = predict_reduce_dim(df, encoder_model, cols_to_reduce)   \n",
    "    df = time_encoding(df)\n",
    "    df = one_hot_encode(df, 'Priority')\n",
    "    df = one_hot_encode(df, 'Status Sparepart')\n",
    "    df = one_hot_encode(df, 'Power_Backup')\n",
    "    if is_train:\n",
    "        df.drop(columns=['ID_Area', 'ID_Mesin_x', 'Machine', 'Breakdown Category',\n",
    "                          'Area', 'ID_Mesin_y', 'Mesin_x', 'Mesin_y', 'Country Machine_x', 'Country Machine_y'], inplace=True)\n",
    "    else:\n",
    "        df.drop(columns=['ID_Area', 'ID_Mesin_x', 'Machine', 'Area', 'ID_Mesin_y', 'Mesin_x', 'Mesin_y',\n",
    "                          'Country Machine_x', 'Country Machine_y'], inplace=True)\n",
    "    for col in df.select_dtypes(include=['category']).columns:\n",
    "        df[col] = df[col].cat.add_categories([-1])\n",
    "        df[col] = df[col].fillna(-1)\n",
    "    df.fillna(-1, inplace=True)\n",
    "    if is_train:\n",
    "        df = smote_transform(df)\n",
    "    return df, encoder_model, cols_to_reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modelling & Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 715us/step - loss: 1.0135 - val_loss: 0.9337\n",
      "Epoch 2/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 672us/step - loss: 0.9194 - val_loss: 0.9270\n",
      "Epoch 3/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 849us/step - loss: 0.9144 - val_loss: 0.9242\n",
      "Epoch 4/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.9123 - val_loss: 0.9225\n",
      "Epoch 5/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.9136 - val_loss: 0.9213\n",
      "Epoch 6/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.9073 - val_loss: 0.9204\n",
      "Epoch 7/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.9102 - val_loss: 0.9197\n",
      "Epoch 8/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 985us/step - loss: 0.9086 - val_loss: 0.9191\n",
      "Epoch 9/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.9091 - val_loss: 0.9186\n",
      "Epoch 10/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 979us/step - loss: 0.9036 - val_loss: 0.9180\n",
      "Epoch 11/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.9101 - val_loss: 0.9174\n",
      "Epoch 12/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.9095 - val_loss: 0.9166\n",
      "Epoch 13/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.9043 - val_loss: 0.9155\n",
      "Epoch 14/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 990us/step - loss: 0.9031 - val_loss: 0.9144\n",
      "Epoch 15/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.9002 - val_loss: 0.9135\n",
      "Epoch 16/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.9065 - val_loss: 0.9131\n",
      "Epoch 17/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.8986 - val_loss: 0.9128\n",
      "Epoch 18/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1000us/step - loss: 0.8988 - val_loss: 0.9126\n",
      "Epoch 19/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.9049 - val_loss: 0.9124\n",
      "Epoch 20/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.8996 - val_loss: 0.9124\n",
      "Epoch 21/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 983us/step - loss: 0.9046 - val_loss: 0.9123\n",
      "Epoch 22/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 0.9038 - val_loss: 0.9121\n",
      "Epoch 23/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.9030 - val_loss: 0.9120\n",
      "Epoch 24/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.8981 - val_loss: 0.9120\n",
      "Epoch 25/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.8964 - val_loss: 0.9118\n",
      "Epoch 26/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 997us/step - loss: 0.8957 - val_loss: 0.9117\n",
      "Epoch 27/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.9036 - val_loss: 0.9117\n",
      "Epoch 28/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 986us/step - loss: 0.9026 - val_loss: 0.9116\n",
      "Epoch 29/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 981us/step - loss: 0.9009 - val_loss: 0.9115\n",
      "Epoch 30/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 969us/step - loss: 0.9016 - val_loss: 0.9115\n",
      "Epoch 31/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 961us/step - loss: 0.9022 - val_loss: 0.9114\n",
      "Epoch 32/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.8967 - val_loss: 0.9114\n",
      "Epoch 33/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 999us/step - loss: 0.8964 - val_loss: 0.9114\n",
      "Epoch 34/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 0.8972 - val_loss: 0.9113\n",
      "Epoch 35/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 0.9038 - val_loss: 0.9113\n",
      "Epoch 36/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 983us/step - loss: 0.9018 - val_loss: 0.9113\n",
      "Epoch 37/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.9001 - val_loss: 0.9112\n",
      "Epoch 38/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 973us/step - loss: 0.8996 - val_loss: 0.9112\n",
      "Epoch 39/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 995us/step - loss: 0.9005 - val_loss: 0.9112\n",
      "Epoch 40/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 957us/step - loss: 0.8999 - val_loss: 0.9111\n",
      "Epoch 41/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 988us/step - loss: 0.9079 - val_loss: 0.9110\n",
      "Epoch 42/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 999us/step - loss: 0.8978 - val_loss: 0.9110\n",
      "Epoch 43/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 955us/step - loss: 0.8988 - val_loss: 0.9110\n",
      "Epoch 44/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 990us/step - loss: 0.8969 - val_loss: 0.9109\n",
      "Epoch 45/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.8995 - val_loss: 0.9109\n",
      "Epoch 46/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 990us/step - loss: 0.8979 - val_loss: 0.9108\n",
      "Epoch 47/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.8975 - val_loss: 0.9108\n",
      "Epoch 48/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.9051 - val_loss: 0.9108\n",
      "Epoch 49/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 962us/step - loss: 0.8965 - val_loss: 0.9108\n",
      "Epoch 50/50\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 940us/step - loss: 0.8950 - val_loss: 0.9107\n",
      "\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 578us/step\n"
     ]
    }
   ],
   "source": [
    "train_df, encoder_model, cols_to_reduce = pipeline(train_df, is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\farel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 742us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 726us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 728us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 581us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 580us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 623us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 606us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 604us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 572us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 802us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 755us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 614us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 624us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 611us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 599us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 592us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 579us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 588us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 577us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 573us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 564us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 573us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 791us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 629us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 630us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 585us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 576us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 630us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 579us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 597us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 623us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 614us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 596us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 620us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 602us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 641us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 767us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 657us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 686us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 652us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 875us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 870us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 745us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 593us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 588us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 598us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 598us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 775us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 590us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 575us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 884us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 590us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 589us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 643us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 628us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 604us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 618us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 778us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 624us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 710us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 637us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 794us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 991us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 583us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 685us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 619us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 914us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 611us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 650us/step\n"
     ]
    }
   ],
   "source": [
    "num_chunks = len(test_df) // 100000 + (len(test_df) % 100000 > 0)\n",
    "chunks = np.array_split(test_df, num_chunks)\n",
    "processed_chunks = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    processed_chunk, _, a = pipeline(chunk, is_train=False, encoder_model=encoder_model, cols_to_reduce=cols_to_reduce)\n",
    "    processed_chunks.append(processed_chunk)\n",
    "\n",
    "testing = pd.concat(processed_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.to_csv(\"test_processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_size = 100_000\n",
    "# chunk_list = []\n",
    "# for chunk in pd.read_csv(\"test_processed.csv\", chunksize=chunk_size, low_memory=False):\n",
    "#     chunk_list.append(chunk)\n",
    "\n",
    "# test_df = pd.concat(chunk_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns='Status')\n",
    "y = train_df['Status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status\n",
       "1    123236\n",
       "0    123236\n",
       "2    123236\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def precision_recall_f1(tp, fp, fn):\n",
    "    # Calculate Precision\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    # Calculate Recall\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    # Calculate F1 Score\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(enable_categorical=True)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.71      0.62     24576\n",
      "           1       0.49      0.42      0.45     24675\n",
      "           2       0.50      0.44      0.47     24691\n",
      "\n",
      "    accuracy                           0.52     73942\n",
      "   macro avg       0.52      0.52      0.52     73942\n",
      "weighted avg       0.52      0.52      0.52     73942\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.53      0.55     24576\n",
      "           1       0.52      0.53      0.53     24675\n",
      "           2       0.53      0.55      0.54     24691\n",
      "\n",
      "    accuracy                           0.54     73942\n",
      "   macro avg       0.54      0.54      0.54     73942\n",
      "weighted avg       0.54      0.54      0.54     73942\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# X = pd.DataFrame(train_df['days_since_last_maintenance'])\n",
    "# y = train_df['Status'].map({'Normal': 0, 'Warning': 1, 'Breakdown': 2})\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(X_train, y_train)\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.79      0.68     24576\n",
      "           1       0.70      0.59      0.64     24675\n",
      "           2       0.71      0.61      0.65     24691\n",
      "\n",
      "    accuracy                           0.66     73942\n",
      "   macro avg       0.67      0.66      0.66     73942\n",
      "weighted avg       0.67      0.66      0.66     73942\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_in_chunks(test_df, model, batch_size=1000, preprocess_fn=None):\n",
    "    chunks = np.array_split(test_df, len(test_df) // batch_size + 1)\n",
    "    predictions = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Processing chunk {i + 1}/{len(chunks)}...\")\n",
    "        preds = model.predict(chunk)\n",
    "        predictions.append(preds)\n",
    "    all_predictions = np.concatenate(predictions, axis=0)\n",
    "    \n",
    "    return all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename col\n",
    "testing.rename(columns={'Power_Backup_ ': 'Power_Backup_Missing'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\farel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/71...\n",
      "Processing chunk 2/71...\n",
      "Processing chunk 3/71...\n",
      "Processing chunk 4/71...\n",
      "Processing chunk 5/71...\n",
      "Processing chunk 6/71...\n",
      "Processing chunk 7/71...\n",
      "Processing chunk 8/71...\n",
      "Processing chunk 9/71...\n",
      "Processing chunk 10/71...\n",
      "Processing chunk 11/71...\n",
      "Processing chunk 12/71...\n",
      "Processing chunk 13/71...\n",
      "Processing chunk 14/71...\n",
      "Processing chunk 15/71...\n",
      "Processing chunk 16/71...\n",
      "Processing chunk 17/71...\n",
      "Processing chunk 18/71...\n",
      "Processing chunk 19/71...\n",
      "Processing chunk 20/71...\n",
      "Processing chunk 21/71...\n",
      "Processing chunk 22/71...\n",
      "Processing chunk 23/71...\n",
      "Processing chunk 24/71...\n",
      "Processing chunk 25/71...\n",
      "Processing chunk 26/71...\n",
      "Processing chunk 27/71...\n",
      "Processing chunk 28/71...\n",
      "Processing chunk 29/71...\n",
      "Processing chunk 30/71...\n",
      "Processing chunk 31/71...\n",
      "Processing chunk 32/71...\n",
      "Processing chunk 33/71...\n",
      "Processing chunk 34/71...\n",
      "Processing chunk 35/71...\n",
      "Processing chunk 36/71...\n",
      "Processing chunk 37/71...\n",
      "Processing chunk 38/71...\n",
      "Processing chunk 39/71...\n",
      "Processing chunk 40/71...\n",
      "Processing chunk 41/71...\n",
      "Processing chunk 42/71...\n",
      "Processing chunk 43/71...\n",
      "Processing chunk 44/71...\n",
      "Processing chunk 45/71...\n",
      "Processing chunk 46/71...\n",
      "Processing chunk 47/71...\n",
      "Processing chunk 48/71...\n",
      "Processing chunk 49/71...\n",
      "Processing chunk 50/71...\n",
      "Processing chunk 51/71...\n",
      "Processing chunk 52/71...\n",
      "Processing chunk 53/71...\n",
      "Processing chunk 54/71...\n",
      "Processing chunk 55/71...\n",
      "Processing chunk 56/71...\n",
      "Processing chunk 57/71...\n",
      "Processing chunk 58/71...\n",
      "Processing chunk 59/71...\n",
      "Processing chunk 60/71...\n",
      "Processing chunk 61/71...\n",
      "Processing chunk 62/71...\n",
      "Processing chunk 63/71...\n",
      "Processing chunk 64/71...\n",
      "Processing chunk 65/71...\n",
      "Processing chunk 66/71...\n",
      "Processing chunk 67/71...\n",
      "Processing chunk 68/71...\n",
      "Processing chunk 69/71...\n",
      "Processing chunk 70/71...\n",
      "Processing chunk 71/71...\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_in_chunks(testing, rf_model, batch_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"dataset/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = pd.DataFrame({\n",
    "    'ID_Transaction': test_id,\n",
    "    'Status': predictions})\n",
    "\n",
    "test_predictions['Status'] = test_predictions['Status'].map({0: 'Normal', 1: 'Warning', 2: 'Breakdown'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status\n",
       "Normal       5740479\n",
       "Breakdown     906504\n",
       "Warning       353017\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions['Status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions.to_csv(\"submission2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.merge(submission, test_predictions, on='ID_Transaction', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission.drop(columns='Status_x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.fillna('Normal', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the column from the predictions DataFrame\n",
    "submission = submission.drop(columns=['Status_x'])  # Adjust the column name as needed\n",
    "\n",
    "# Optionally rename 'Status_y' to 'Target' (or your desired name)\n",
    "submission.rename(columns={'Status_y': 'Status'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Result & Analysis**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
